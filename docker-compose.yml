services:
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"       # API
      - "9001:9001"       # Console UI
    environment:
      MINIO_ACCESS_KEY: admin
      MINIO_SECRET_KEY: admin123
    command: server /data --console-address ":9001"
    volumes:
      - ./minio_data:/data
    networks:
      - datalake-cluster

  spark-master:
    image: datascience-env
    container_name: spark-master
    ports:
      - "8888:8888"  # Jupyter Notebook
      - "8887:8080"  # Spark Master UI (reliÃ© au port interne 8080)
      - "4040:4040"  # Spark job UI
      - "7077:7077"  # port Spark master (RPC pour les workers)
    environment:
      - SPARK_MODE=master
    networks:
      - datalake-cluster
    command: /entrypoint.sh

  spark-worker1:
    image: datascience-env
    container_name: spark-worker1
    environment:
      - SPARK_MODE=worker
    networks:
      - datalake-cluster
    command: /entrypoint.sh

  spark-worker2:
    image: datascience-env
    container_name: spark-worker2
    environment:
      - SPARK_MODE=worker
    networks:
      - datalake-cluster
    command: /entrypoint.sh

  airflow:
    image: apache/airflow:2.7.3-python3.10
    container_name: airflow
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__WEBSERVER__AUTH_BACKEND: airflow.www.security.auth_backend.password_auth
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin123
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8081:8080"
    command: >
      bash -c "airflow db init &&
               airflow users create --username admin --password admin123 --firstname Admin --lastname User --role Admin --email admin@example.com &&
               airflow webserver"
    networks:
      - datalake-cluster

networks:
  datalake-cluster:
    external: true